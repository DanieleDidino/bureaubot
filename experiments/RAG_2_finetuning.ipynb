{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d28930a-8b8b-44e6-8097-c99375bf37c9",
   "metadata": {},
   "source": [
    "# Fine tuning\n",
    "\n",
    "Reference: [Fine-Tuning Embeddings for RAG with Synthetic Data](https://medium.com/llamaindex-blog/fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971)\n",
    "\n",
    "Finetune an opensource sentencetransformers embedding model on our synthetically generated dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6340211a-db64-458f-b3fb-4b87f09ab743",
   "metadata": {},
   "source": [
    "## Load pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bd53aa4c-b83b-41a3-9b73-498aca71e2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers import losses\n",
    "from sentence_transformers.evaluation import InformationRetrievalEvaluator\n",
    "\n",
    "import json\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers import InputExample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70df8f69-9d40-4467-9755-5582b4ca5178",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)ab102/.gitattributes: 100%|██████████| 1.52k/1.52k [00:00<00:00, 4.64MB/s]\n",
      "Downloading (…)_Pooling/config.json: 100%|██████████| 190/190 [00:00<00:00, 694kB/s]\n",
      "Downloading (…)2d2d7ab102/README.md: 100%|██████████| 78.9k/78.9k [00:00<00:00, 637kB/s]\n",
      "Downloading (…)2d7ab102/config.json: 100%|██████████| 684/684 [00:00<00:00, 2.66MB/s]\n",
      "Downloading (…)ce_transformers.json: 100%|██████████| 124/124 [00:00<00:00, 593kB/s]\n",
      "Downloading model.safetensors: 100%|██████████| 133M/133M [00:18<00:00, 7.38MB/s] \n",
      "Downloading pytorch_model.bin: 100%|██████████| 134M/134M [00:14<00:00, 9.40MB/s] \n",
      "Downloading (…)nce_bert_config.json: 100%|██████████| 52.0/52.0 [00:00<00:00, 181kB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 125/125 [00:00<00:00, 459kB/s]\n",
      "Downloading (…)ab102/tokenizer.json: 100%|██████████| 711k/711k [00:00<00:00, 1.65MB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 366/366 [00:00<00:00, 1.42MB/s]\n",
      "Downloading (…)2d2d7ab102/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 1.60MB/s]\n",
      "Downloading (…)d7ab102/modules.json: 100%|██████████| 229/229 [00:00<00:00, 334kB/s]\n"
     ]
    }
   ],
   "source": [
    "model_id = \"BAAI/bge-small-en\"\n",
    "model = SentenceTransformer(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3502aaba-3e8a-42a6-9665-9d59304e798b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': True}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': True, 'pooling_mode_mean_tokens': False, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fd0ab4-6687-4b54-aff7-b30aa96f3698",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e26aa72e-c158-4d6c-862f-bb555e5e456d",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATASET_FPATH = 'afa_docs/train_val_data/train_dataset.json'\n",
    "VAL_DATASET_FPATH = 'afa_docs/train_val_data/val_dataset.json'\n",
    "\n",
    "# We use a very small batchsize to run this toy example on a local machine. \n",
    "# This should typically be much larger.\n",
    "BATCH_SIZE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2713df64-b708-48f9-8a0a-505ee24ec381",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TRAIN_DATASET_FPATH, 'r+') as f:\n",
    "    train_dataset = json.load(f)\n",
    "\n",
    "with open(VAL_DATASET_FPATH, 'r+') as f:\n",
    "    val_dataset = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "647d5d8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['queries', 'corpus', 'relevant_docs'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f07e006a-9a08-4961-b639-66a7a65bc603",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = train_dataset\n",
    "\n",
    "corpus = dataset['corpus']\n",
    "queries = dataset['queries']\n",
    "relevant_docs = dataset['relevant_docs']\n",
    "\n",
    "examples = []\n",
    "for query_id, query in queries.items():\n",
    "    node_id = relevant_docs[query_id][0]\n",
    "    text = corpus[node_id]\n",
    "    example = InputExample(texts=[query, text])\n",
    "    examples.append(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "75ac1e31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7b7f2f2-0012-4ec7-8f44-1909d42c84b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(\n",
    "    examples, batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8acef2-8618-4afa-84e0-cd959d181208",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d643c3f-563a-44ae-a4aa-6a5fd8780ff9",
   "metadata": {},
   "source": [
    "`MultipleNegativesRankingLoss` is a great loss function if you only have positive pairs, for example, only pairs of similar texts like pairs of paraphrases, pairs of duplicate questions, pairs of (query, response), or pairs of (source_language, target_language).\n",
    "\n",
    "This loss function works great to train embeddings for retrieval setups where you have positive pairs (e.g. (query, relevant_doc)) as it will sample in each batch n-1 negative docs randomly.\n",
    "\n",
    "The performance usually increases with increasing batch sizes.\n",
    "\n",
    "For more detals, see:\n",
    "- [docs](https://www.sbert.net/docs/package_reference/losses.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "928eec1f-0f47-4bac-9189-7b791e81024d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = losses.MultipleNegativesRankingLoss(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95044745-c271-40b3-a6d9-dd646ab281de",
   "metadata": {},
   "source": [
    "## Evaluator \n",
    "\n",
    "We setup an evaluator with our val split of the dataset to monitor how well the embedding model is performing during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "64ab1591-ce7e-4990-a708-9b22a7ce0944",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = val_dataset\n",
    "\n",
    "corpus = dataset['corpus']\n",
    "queries = dataset['queries']\n",
    "relevant_docs = dataset['relevant_docs']\n",
    "\n",
    "evaluator = InformationRetrievalEvaluator(queries, corpus, relevant_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5deb95-6d9a-4da8-ba46-f0317b12d6df",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "The training loop is very straight forward to set up thanks to `sentencetransformers` high-level model training API.\n",
    "All we need to do is plugging in the data loader, loss function, and evaluator that we defined in the previous cells (along with a couple of additional minor settings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "99ff9b09-c191-4ac0-a89e-629031e648d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We train the model for very few epochs in this toy example.\n",
    "# This should typically be higher for better performance.\n",
    "EPOCHS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f3b6afdf-87d6-40be-b6fd-36f89dbb3612",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 3/3 [00:06<00:00,  2.06s/it]\n",
      "Iteration: 100%|██████████| 3/3 [00:05<00:00,  1.91s/it]\n",
      "Epoch: 100%|██████████| 2/2 [00:17<00:00,  8.76s/it]\n"
     ]
    }
   ],
   "source": [
    "warmup_steps = int(len(loader) * EPOCHS * 0.1)\n",
    "\n",
    "model.fit(\n",
    "    train_objectives=[(loader, loss)],\n",
    "    epochs=EPOCHS,\n",
    "    warmup_steps=warmup_steps,\n",
    "    output_path='exp_finetune',\n",
    "    show_progress_bar=True,\n",
    "    evaluator=evaluator, \n",
    "    evaluation_steps=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6304d1f-aecf-42ff-9852-03f49bde8f2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
