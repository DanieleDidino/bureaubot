{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\") \n",
    "openai.api_key = openai_api_key\n",
    "\n",
    "# Search\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "GOOGLE_CSE_ID = os.getenv(\"GOOGLE_CSE_ID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using LlamaIndex as a Callable Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.web_research:Generating questions for Google Search ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mThought: Do I need to use a tool? Yes\n",
      "Action: Websearch\n",
      "Action Input: Duties of landlords in Berlin, Germany\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.web_research:Questions for Google Search (raw): {'question': 'Duties of landlords in Berlin, Germany', 'text': LineList(lines=['1. What are the legal obligations of landlords in Berlin, Germany?\\n', '2. What are the responsibilities of landlords in Berlin, Germany?\\n', '3. What are the duties and requirements for landlords in Berlin, Germany?'])}\n",
      "INFO:langchain.retrievers.web_research:Questions for Google Search: ['1. What are the legal obligations of landlords in Berlin, Germany?\\n', '2. What are the responsibilities of landlords in Berlin, Germany?\\n', '3. What are the duties and requirements for landlords in Berlin, Germany?']\n",
      "INFO:langchain.retrievers.web_research:Searching for relevant urls...\n",
      "INFO:langchain.retrievers.web_research:Searching for relevant urls...\n",
      "INFO:langchain.retrievers.web_research:Search results: [{'title': 'Basic Law for the Federal Republic of Germany', 'link': 'https://www.gesetze-im-internet.de/englisch_gg/englisch_gg.html', 'snippet': '(1) If a Land fails to comply with its obligations under this Basic Law or other federal laws, the Federal Government, with the consent of the Bundesrat, may\\xa0...'}]\n",
      "INFO:langchain.retrievers.web_research:Searching for relevant urls...\n",
      "INFO:langchain.retrievers.web_research:Search results: [{'title': 'Basic Law for the Federal Republic of Germany', 'link': 'https://www.gesetze-im-internet.de/englisch_gg/englisch_gg.html', 'snippet': '(1) Berlin is the capital of the Federal Republic of Germany. The ... (2) The Federation shall discharge rail transport administration responsibilities\\xa0...'}]\n",
      "INFO:langchain.retrievers.web_research:Searching for relevant urls...\n",
      "INFO:langchain.retrievers.web_research:Search results: [{'title': 'Basic Law for the Federal Republic of Germany', 'link': 'https://www.gesetze-im-internet.de/englisch_gg/englisch_gg.html', 'snippet': '... needed to discharge their responsibilities. (2) The Federation and the Länder ... requirements of the fifth sentence of paragraph (3) of Article 109. In the\\xa0...'}]\n",
      "INFO:langchain.retrievers.web_research:New URLs to load: ['https://www.gesetze-im-internet.de/englisch_gg/englisch_gg.html']\n",
      "INFO:langchain.retrievers.web_research:Indexing new urls...\n",
      "Fetching pages: 100%|##########| 1/1 [00:00<00:00,  4.02it/s]\n",
      "INFO:langchain.retrievers.web_research:Grabbing most relevant splits from urls...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Observation: \u001b[33;1m\u001b[1;3m{'question': 'Duties of landlords in Berlin, Germany', 'answer': 'The duties of landlords in Berlin, Germany include restrictions on rent increases and the obligation to provide compensation payments under certain circumstances. Rent increases are capped at a maximum of 20% within three years, or 15% in areas with a jeopardized housing supply. However, these restrictions do not apply if the rent-control scheme has ceased to apply or if the increase does not exceed the amount of the most recently payable compensation payment. Landlords may also demand information from tenants regarding compensation payments. These regulations are outlined in the German Civil Code (Bürgerliches Gesetzbuch). \\n', 'sources': ''}\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "from langchain.agents import AgentType, Tool, initialize_agent\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chat_models.openai import ChatOpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.retrievers.web_research import WebResearchRetriever\n",
    "from langchain.utilities import GoogleSearchAPIWrapper\n",
    "from langchain.vectorstores import Chroma\n",
    "from llama_index import Prompt, StorageContext, load_index_from_storage\n",
    "\n",
    "# logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "# logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "logging.basicConfig()\n",
    "logging.getLogger(\"langchain.retrievers.web_research\").setLevel(logging.INFO)\n",
    "\n",
    "qa_template = Prompt(\n",
    "    \"We have provided context information below. \\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\"\n",
    "    \"\\n---------------------\\n\"\n",
    "    \"Do not give me an answer if it is not mentioned in the context as a fact.\\n\"\n",
    "    \"Always state where the information was found, and if possible where more information can be accessed.\\n\"\n",
    "    \"Given this information, please provide me with an answer to the following question:\\n\"\n",
    "    \"\\n{query_str}\\n\"\n",
    ")\n",
    "\n",
    "# LLM\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "## Index tool from documents in vector store\n",
    "storage_context = StorageContext.from_defaults(persist_dir=\"vector_db\")\n",
    "index = load_index_from_storage(storage_context)\n",
    "\n",
    "query_engine = index.as_query_engine(\n",
    "    streaming=True, text_qa_template=qa_template, similarity_top_k=5\n",
    ")\n",
    "\n",
    "## Websearch Tool\n",
    "# Vectorstore\n",
    "vectorstore = Chroma(\n",
    "    embedding_function=OpenAIEmbeddings(), persist_directory=\"./chroma_db_oai\"\n",
    ")\n",
    "\n",
    "# Search\n",
    "search = GoogleSearchAPIWrapper()\n",
    "\n",
    "# Initialize\n",
    "web_research_retriever = WebResearchRetriever.from_llm(\n",
    "    vectorstore=vectorstore,\n",
    "    llm=llm,\n",
    "    search=search,\n",
    ")\n",
    "\n",
    "# chain\n",
    "qa_chain = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    llm, retriever=web_research_retriever\n",
    ")\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"LlamaIndex\",\n",
    "        func=lambda q: str(query_engine.query(q)),\n",
    "        description=\"useful for when you want to answer questions about work and unemployment. The input to this tool should be a complete english sentence.\",\n",
    "        return_direct=True,\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Websearch\",\n",
    "        func=lambda q: str(qa_chain({\"question\": q})),\n",
    "        description=\"useful for when the LlamaIndex Tool does not have a sufficient answer in it's documents and you want to answer more general questions concerning laws and administration in Berlin, Germany. The input to this tool should be a complete english sentence.\",\n",
    "        return_direct=True,\n",
    "    ),\n",
    "]\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "\n",
    "agent_executor = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    "    memory=memory,\n",
    ")\n",
    "\n",
    "result = agent_executor.run(\n",
    "    input=\"Where can I find information about the duties of landlords?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extend the Search and provide Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching pages: 100%|##########| 2/2 [00:00<00:00, 20.26it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from typing import List\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.chat_models.openai import ChatOpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.output_parsers.pydantic import PydanticOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.retrievers.web_research import WebResearchRetriever\n",
    "from langchain.utilities import GoogleSearchAPIWrapper\n",
    "from langchain.vectorstores import Chroma\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# LLMChain\n",
    "search_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"You are an assistant tasked with improving Google search \n",
    "    results. Generate THREE Google search queries that are similar to\n",
    "    this question. The output should be a numbered list of questions and each\n",
    "    should have a question mark at the end: {question}\"\"\",\n",
    ")\n",
    "\n",
    "\n",
    "class LineList(BaseModel):\n",
    "    \"\"\"List of questions.\"\"\"\n",
    "\n",
    "    lines: List[str] = Field(description=\"Questions\")\n",
    "\n",
    "\n",
    "class QuestionListOutputParser(PydanticOutputParser):\n",
    "    \"\"\"Output parser for a list of numbered questions.\"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__(pydantic_object=LineList)\n",
    "\n",
    "    def parse(self, text: str) -> LineList:\n",
    "        lines = re.findall(r\"\\d+\\..*?\\n\", text)\n",
    "        return LineList(lines=lines)\n",
    "\n",
    "# LLM\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=search_prompt,\n",
    "    output_parser=QuestionListOutputParser(),\n",
    ")\n",
    "\n",
    "\n",
    "# Vectorstore\n",
    "vectorstore = Chroma(\n",
    "    embedding_function=OpenAIEmbeddings(), persist_directory=\"./chroma_db_oai\"\n",
    ")\n",
    "\n",
    "# Search\n",
    "search = GoogleSearchAPIWrapper()\n",
    "\n",
    "# Initialize\n",
    "web_research_retriever_llm_chain = WebResearchRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    llm_chain=llm_chain,\n",
    "    search=search,\n",
    ")\n",
    "\n",
    "def run_chain(user_input):\n",
    "    docs = web_research_retriever_llm_chain.get_relevant_documents(user_input)\n",
    "    chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
    "    output = chain( {\"input_documents\": docs, \"question\": user_input}, return_only_outputs=True )\n",
    "    return output[\"output_text\"], docs\n",
    "\n",
    "\n",
    "user_input = \"Where can I find information about the duties of landlords in Berlin?\"\n",
    "output, docs = run_chain(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You can find information about the duties of landlords in Berlin in Chapter 2, Subchapter 1, Section 556 of the Berlin Rent Act. This section specifically addresses agreements on operating costs and outlines the responsibilities of the owner or holder of the heritable building right. Additionally, you may also find relevant information in Subtitle 4, Section 581, which discusses contractual duties typical for a usufructuary lease.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: https://www.gesetze-im-internet.de/englisch_bgb/englisch_bgb.html\n",
      "Title: German Civil Code BGB\n",
      "Page Content: Chapter 2  \n",
      "Rent\n",
      "\n",
      "table of contents\n",
      "\n",
      "Subchapter 1  \n",
      "Agreements on rent\n",
      "\n",
      "table of contents\n",
      "\n",
      "Section 556  \n",
      "Agreements on operating costs\n",
      "\n",
      "(1) The contractual parties may agree that the lessee is to bear the operating\n",
      "costs. Operating costs are the costs that are incurred on an ongoing basis by\n",
      "the owner or the holder of the heritable building right as a result of the\n",
      "ownership of or the heritable building right to the plot of land or as a\n",
      "result of the use of the building, the outbuildings, facilities, installations\n",
      "and the land in accordance with the purpose for which they are intended. The\n",
      "drawing up of the statement of operating costs continues to be governed by the\n",
      "Ordinance on Operating Costs ( _Betriebskostenverordnung_ ) of 25 November\n",
      "2003 (Federal Law Gazette I pp. 2346, 2347). The Federal Government is\n",
      "authorised to enact provisions on the drawing up of the statement of operating\n",
      "costs by statutory instrument without the approval of the Bundesrat.\n",
      "\n",
      "(2) The contractual parties may agree, subject to other provisions, that\n",
      "operating costs may be reported as a lump sum or as an advance payment.\n",
      "Advance payments for operating costs may only be agreed in a reasonable\n",
      "amount.\n",
      "-----\n",
      "Source: https://www.gesetze-im-internet.de/englisch_bgb/englisch_bgb.html\n",
      "Title: German Civil Code BGB\n",
      "Page Content: Section 556  \n",
      "Agreements on operating costs\n",
      "\n",
      "(1) The contractual parties may agree that the lessee is to bear the operating\n",
      "costs. Operating costs are the costs that are incurred on an ongoing basis by\n",
      "the owner or the holder of the heritable building right as a result of the\n",
      "ownership of or the heritable building right to the plot of land or as a\n",
      "result of the use of the building, the outbuildings, facilities, installations\n",
      "and the land in accordance with the purpose for which they are intended. The\n",
      "drawing up of the statement of operating costs continues to be governed by the\n",
      "Ordinance on Operating Costs ( _Betriebskostenverordnung_ ) of 25 November\n",
      "2003 (Federal Law Gazette I pp. 2346, 2347). The Federal Government is\n",
      "authorised to enact provisions on the drawing up of the statement of operating\n",
      "costs by statutory instrument without the approval of the Bundesrat.\n",
      "\n",
      "(2) The contractual parties may agree, subject to other provisions, that\n",
      "operating costs may be reported as a lump sum or as an advance payment.\n",
      "Advance payments for operating costs may only be agreed in a reasonable\n",
      "amount.\n",
      "-----\n",
      "Source: https://service.berlin.de/dienstleistung/324659/en/\n",
      "Title: Applying for an EU Blue Card - Services - Service Berlin - Berlin.de\n",
      "Page Content: * Sofortprogramm des Senats\n",
      "    * Ukraine\n",
      "    * Energiekrise\n",
      "    * Pressemitteilungen\n",
      "    * Polizeimeldungen\n",
      "    * Veranstaltungen\n",
      "  * ### Themen\n",
      "\n",
      "    * Gemeinsam Digital\n",
      "    * Grundsteuer\n",
      "    * Mobilitatswende\n",
      "    * Moderne Verwaltung\n",
      "    * Berliner Mauer\n",
      "    * Berlin im Überblick\n",
      "  * ### Weitere Informationen\n",
      "\n",
      "    * Kultur & Ausgehen\n",
      "    * Tourismus\n",
      "    * Wirtschaft\n",
      "    * Stadtleben\n",
      "    * BerlinFinder\n",
      "    * Stadtplan\n",
      "\n",
      "Berlin.de is a service of the State of Berlin.\n",
      "-----\n",
      "Source: https://www.gesetze-im-internet.de/englisch_bgb/englisch_bgb.html\n",
      "Title: German Civil Code BGB\n",
      "Page Content: 2\\. if the rent is assessed by weeks, at the latest on the first working day\n",
      "of a week with effect for the end of the following Saturday;\n",
      "\n",
      "3\\. if the rent is assessed in months or longer periods of time, at the latest\n",
      "on the third working day of a calendar month with effect for the end of the\n",
      "second month thereafter; in the case of a lease of commercially used\n",
      "undeveloped plots of land, however, only to the end of a calendar quarter.\n",
      "\n",
      "(2) In the case of a lease of business premises, giving notice of termination\n",
      "in accordance with usual procedure is permissible at the latest on the third\n",
      "working day of a calendar quarter with effect for the end of the next calendar\n",
      "quarter.\n",
      "\n",
      "(3) In the case of a lease of movable things or of digital products, giving\n",
      "notice of termination in accordance with usual procedure is permissible\n",
      "\n",
      "1\\. if the rent is assessed by days, on any day with effect for the end of the\n",
      "following day;\n",
      "\n",
      "2\\. if the rent is assessed by longer periods of time, at the latest on the\n",
      "third day prior to the day at the end of which the lease is to terminate.\n",
      "\n",
      "The provisions on the termination of consumer contracts on digital products\n",
      "remain unaffected.\n",
      "\n",
      "(4) Subsection (1) no. 3, subsections (2) and (3) no. 2 also are to be applied\n",
      "if a lease may be terminated for cause observing the statutory notice period.\n",
      "\n",
      "table of contents\n",
      "\n",
      "Subtitle 4  \n",
      "Usufructuary lease\n",
      "\n",
      "table of contents\n",
      "\n",
      "Section 581  \n",
      "Contractual duties typical for a usufructuary lease\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "for doc in docs:\n",
    "    page_content = doc.page_content\n",
    "    metadata = doc.metadata\n",
    "    source = metadata['source']\n",
    "    title = metadata['title']\n",
    "    \n",
    "    # Process or use the extracted information as needed\n",
    "    print(f\"Source: {source}\")\n",
    "    print(f\"Title: {title}\")\n",
    "    print(f\"Page Content: {page_content}\")\n",
    "    print(\"-----\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent with Tools - openai cookbook\n",
    "https://cookbook.openai.com/examples/how_to_build_a_tool-using_agent_with_langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Union\n",
    "import re\n",
    "import openai\n",
    "import pandas as pd\n",
    "from langchain import LLMChain, OpenAI, SerpAPIWrapper\n",
    "\n",
    "# Langchain imports\n",
    "from langchain.agents import (\n",
    "    AgentExecutor,\n",
    "    AgentOutputParser,\n",
    "    LLMSingleActionAgent,\n",
    "    Tool,\n",
    ")\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# LLM wrapper\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Embeddings and vectorstore\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "# Conversational memory\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.prompts import BaseChatPromptTemplate, ChatPromptTemplate\n",
    "from langchain.schema import AgentAction, AgentFinish, HumanMessage, SystemMessage\n",
    "from langchain.vectorstores import Pinecone\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## template with history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a prompt template which can interpolate the history\n",
    "template_with_history = \"\"\"You are BureauBot, a professional search engine who provides informative answers to users. Answer the following questions as best you can. You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Begin! Remember to give detailed, informative answers\n",
    "\n",
    "Previous conversation history:\n",
    "{history}\n",
    "\n",
    "New question: {input}\n",
    "{agent_scratchpad}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## template without history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the prompt with input variables for tools, user input and a scratchpad for the model to record its workings\n",
    "template = \"\"\"Answer the following questions as best you can, but speaking as a pirate might speak. You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Begin! Remember to speak as a pirate when giving your final answer. Use lots of \"Arg\"s\n",
    "\n",
    "Question: {input}\n",
    "{agent_scratchpad}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## custom prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a prompt template\n",
    "class CustomPromptTemplate(BaseChatPromptTemplate):\n",
    "    # The template to use\n",
    "    template: str\n",
    "    # The list of tools available\n",
    "    tools: List[Tool]\n",
    "    \n",
    "    def format_messages(self, **kwargs) -> str:\n",
    "        # Get the intermediate steps (AgentAction, Observation tuples)\n",
    "        \n",
    "        # Format them in a particular way\n",
    "        intermediate_steps = kwargs.pop(\"intermediate_steps\")\n",
    "        thoughts = \"\"\n",
    "        for action, observation in intermediate_steps:\n",
    "            thoughts += action.log\n",
    "            thoughts += f\"\\nObservation: {observation}\\nThought: \"\n",
    "            \n",
    "        # Set the agent_scratchpad variable to that value\n",
    "        kwargs[\"agent_scratchpad\"] = thoughts\n",
    "        \n",
    "        # Create a tools variable from the list of tools provided\n",
    "        kwargs[\"tools\"] = \"\\n\".join([f\"{tool.name}: {tool.description}\" for tool in self.tools])\n",
    "        \n",
    "        # Create a list of tool names for the tools provided\n",
    "        kwargs[\"tool_names\"] = \", \".join([tool.name for tool in self.tools])\n",
    "        formatted = self.template.format(**kwargs)\n",
    "        return [HumanMessage(content=formatted)]\n",
    "    \n",
    "prompt = CustomPromptTemplate(\n",
    "    template=template,\n",
    "    tools=tools,\n",
    "    # This omits the `agent_scratchpad`, `tools`, and `tool_names` variables because those are generated dynamically\n",
    "    # This includes the `intermediate_steps` variable because that is needed\n",
    "    input_variables=[\"input\", \"intermediate_steps\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## custom output parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomOutputParser(AgentOutputParser):\n",
    "    \n",
    "    def parse(self, llm_output: str) -> Union[AgentAction, AgentFinish]:\n",
    "        \n",
    "        # Check if agent should finish\n",
    "        if \"Final Answer:\" in llm_output:\n",
    "            return AgentFinish(\n",
    "                # Return values is generally always a dictionary with a single `output` key\n",
    "                # It is not recommended to try anything else at the moment :)\n",
    "                return_values={\"output\": llm_output.split(\"Final Answer:\")[-1].strip()},\n",
    "                log=llm_output,\n",
    "            )\n",
    "        \n",
    "        # Parse out the action and action input\n",
    "        regex = r\"Action: (.*?)[\\n]*Action Input:[\\s]*(.*)\"\n",
    "        match = re.search(regex, llm_output, re.DOTALL)\n",
    "        \n",
    "        # If it can't parse the output it raises an error\n",
    "        # You can add your own logic here to handle errors in a different way i.e. pass to a human, give a canned response\n",
    "        if not match:\n",
    "            raise ValueError(f\"Could not parse LLM output: `{llm_output}`\")\n",
    "        action = match.group(1).strip()\n",
    "        action_input = match.group(2)\n",
    "        \n",
    "        # Return the action and action input\n",
    "        return AgentAction(tool=action, tool_input=action_input.strip(\" \").strip('\"'), log=llm_output)\n",
    "    \n",
    "output_parser = CustomOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "Loading all indices.\n",
      "Loading all indices.\n",
      "Loading all indices.\n",
      "Loading all indices.\n"
     ]
    }
   ],
   "source": [
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "## Index tool from documents in vector store\n",
    "storage_context = StorageContext.from_defaults(persist_dir=\"vector_db\")\n",
    "index = load_index_from_storage(storage_context)\n",
    "\n",
    "query_engine = index.as_query_engine(\n",
    "    streaming=True, \n",
    "    text_qa_template=qa_template, \n",
    "    similarity_top_k=5\n",
    ")\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"LlamaIndex\",\n",
    "        func=lambda q: str(query_engine.query(q)),\n",
    "        description=\"useful for when you want to answer questions about work and unemployment. The input to this tool should be a complete english sentence.\",\n",
    "        return_direct=True,\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Websearch\",\n",
    "        func=lambda q: str(qa_chain({\"question\": q})),\n",
    "        description=\"useful for when you want to answer questions about work and unemployment and the LlamaIndex Tool does not have a sufficient answer in it's documents. The input to this tool should be a complete english sentence.\",\n",
    "        return_direct=True,\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Re-initialize the agent with our new list of tools\n",
    "prompt_with_history = CustomPromptTemplate(\n",
    "    template=template_with_history,\n",
    "    tools=tools,\n",
    "    input_variables=[\"input\", \"intermediate_steps\", \"history\"]\n",
    ")\n",
    "\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt_with_history)\n",
    "multi_tool_names = [tool.name for tool in tools]\n",
    "multi_tool_agent = LLMSingleActionAgent(\n",
    "    llm_chain=llm_chain, \n",
    "    output_parser=output_parser,\n",
    "    stop=[\"\\nObservation:\"], \n",
    "    allowed_tools=multi_tool_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_tool_memory = ConversationBufferWindowMemory(k=2)\n",
    "\n",
    "multi_tool_executor = AgentExecutor.from_agent_and_tools(\n",
    "    agent=multi_tool_agent, tools=tools, verbose=True, memory=multi_tool_memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Thought: I should look for information about the duties of landlords\n",
      "Action: Websearch\n",
      "Action Input: duties of landlords\u001b[0mINFO:langchain.retrievers.web_research:Generating questions for Google Search ...\n",
      "Generating questions for Google Search ...\n",
      "Generating questions for Google Search ...\n",
      "Generating questions for Google Search ...\n",
      "Generating questions for Google Search ...\n",
      "Generating questions for Google Search ...\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:langchain.retrievers.web_research:Questions for Google Search (raw): {'question': 'duties of landlords', 'text': LineList(lines=['1. What are the responsibilities and duties of landlords?\\n', '2. What are the obligations and tasks that landlords are required to fulfill?\\n', '3. What are the roles and responsibilities that landlords have to undertake?'])}\n",
      "Questions for Google Search (raw): {'question': 'duties of landlords', 'text': LineList(lines=['1. What are the responsibilities and duties of landlords?\\n', '2. What are the obligations and tasks that landlords are required to fulfill?\\n', '3. What are the roles and responsibilities that landlords have to undertake?'])}\n",
      "Questions for Google Search (raw): {'question': 'duties of landlords', 'text': LineList(lines=['1. What are the responsibilities and duties of landlords?\\n', '2. What are the obligations and tasks that landlords are required to fulfill?\\n', '3. What are the roles and responsibilities that landlords have to undertake?'])}\n",
      "Questions for Google Search (raw): {'question': 'duties of landlords', 'text': LineList(lines=['1. What are the responsibilities and duties of landlords?\\n', '2. What are the obligations and tasks that landlords are required to fulfill?\\n', '3. What are the roles and responsibilities that landlords have to undertake?'])}\n",
      "Questions for Google Search (raw): {'question': 'duties of landlords', 'text': LineList(lines=['1. What are the responsibilities and duties of landlords?\\n', '2. What are the obligations and tasks that landlords are required to fulfill?\\n', '3. What are the roles and responsibilities that landlords have to undertake?'])}\n",
      "Questions for Google Search (raw): {'question': 'duties of landlords', 'text': LineList(lines=['1. What are the responsibilities and duties of landlords?\\n', '2. What are the obligations and tasks that landlords are required to fulfill?\\n', '3. What are the roles and responsibilities that landlords have to undertake?'])}\n",
      "INFO:langchain.retrievers.web_research:Questions for Google Search: ['1. What are the responsibilities and duties of landlords?\\n', '2. What are the obligations and tasks that landlords are required to fulfill?\\n', '3. What are the roles and responsibilities that landlords have to undertake?']\n",
      "Questions for Google Search: ['1. What are the responsibilities and duties of landlords?\\n', '2. What are the obligations and tasks that landlords are required to fulfill?\\n', '3. What are the roles and responsibilities that landlords have to undertake?']\n",
      "Questions for Google Search: ['1. What are the responsibilities and duties of landlords?\\n', '2. What are the obligations and tasks that landlords are required to fulfill?\\n', '3. What are the roles and responsibilities that landlords have to undertake?']\n",
      "Questions for Google Search: ['1. What are the responsibilities and duties of landlords?\\n', '2. What are the obligations and tasks that landlords are required to fulfill?\\n', '3. What are the roles and responsibilities that landlords have to undertake?']\n",
      "Questions for Google Search: ['1. What are the responsibilities and duties of landlords?\\n', '2. What are the obligations and tasks that landlords are required to fulfill?\\n', '3. What are the roles and responsibilities that landlords have to undertake?']\n",
      "Questions for Google Search: ['1. What are the responsibilities and duties of landlords?\\n', '2. What are the obligations and tasks that landlords are required to fulfill?\\n', '3. What are the roles and responsibilities that landlords have to undertake?']\n",
      "INFO:langchain.retrievers.web_research:Searching for relevant urls...\n",
      "Searching for relevant urls...\n",
      "Searching for relevant urls...\n",
      "Searching for relevant urls...\n",
      "Searching for relevant urls...\n",
      "Searching for relevant urls...\n",
      "INFO:langchain.retrievers.web_research:Searching for relevant urls...\n",
      "Searching for relevant urls...\n",
      "Searching for relevant urls...\n",
      "Searching for relevant urls...\n",
      "Searching for relevant urls...\n",
      "Searching for relevant urls...\n",
      "INFO:langchain.retrievers.web_research:Search results: [{'title': 'Directive 2012/27/EU of the European Parliament and of the Council ...', 'link': 'https://eur-lex.europa.eu/LexUriServ/LexUriServ.do?uri=OJ:L:2012:315:0001:0056:en:PDF', 'snippet': '25.10.2012 ... ... role of central government buildings laid down in paragraphs 1, 5 and 6; ... integrated approach to reduce CO2 emissions from light-duty vehicles\\xa0...'}]\n",
      "Search results: [{'title': 'Directive 2012/27/EU of the European Parliament and of the Council ...', 'link': 'https://eur-lex.europa.eu/LexUriServ/LexUriServ.do?uri=OJ:L:2012:315:0001:0056:en:PDF', 'snippet': '25.10.2012 ... ... role of central government buildings laid down in paragraphs 1, 5 and 6; ... integrated approach to reduce CO2 emissions from light-duty vehicles\\xa0...'}]\n",
      "Search results: [{'title': 'Directive 2012/27/EU of the European Parliament and of the Council ...', 'link': 'https://eur-lex.europa.eu/LexUriServ/LexUriServ.do?uri=OJ:L:2012:315:0001:0056:en:PDF', 'snippet': '25.10.2012 ... ... role of central government buildings laid down in paragraphs 1, 5 and 6; ... integrated approach to reduce CO2 emissions from light-duty vehicles\\xa0...'}]\n",
      "Search results: [{'title': 'Directive 2012/27/EU of the European Parliament and of the Council ...', 'link': 'https://eur-lex.europa.eu/LexUriServ/LexUriServ.do?uri=OJ:L:2012:315:0001:0056:en:PDF', 'snippet': '25.10.2012 ... ... role of central government buildings laid down in paragraphs 1, 5 and 6; ... integrated approach to reduce CO2 emissions from light-duty vehicles\\xa0...'}]\n",
      "Search results: [{'title': 'Directive 2012/27/EU of the European Parliament and of the Council ...', 'link': 'https://eur-lex.europa.eu/LexUriServ/LexUriServ.do?uri=OJ:L:2012:315:0001:0056:en:PDF', 'snippet': '25.10.2012 ... ... role of central government buildings laid down in paragraphs 1, 5 and 6; ... integrated approach to reduce CO2 emissions from light-duty vehicles\\xa0...'}]\n",
      "Search results: [{'title': 'Directive 2012/27/EU of the European Parliament and of the Council ...', 'link': 'https://eur-lex.europa.eu/LexUriServ/LexUriServ.do?uri=OJ:L:2012:315:0001:0056:en:PDF', 'snippet': '25.10.2012 ... ... role of central government buildings laid down in paragraphs 1, 5 and 6; ... integrated approach to reduce CO2 emissions from light-duty vehicles\\xa0...'}]\n",
      "INFO:langchain.retrievers.web_research:Searching for relevant urls...\n",
      "Searching for relevant urls...\n",
      "Searching for relevant urls...\n",
      "Searching for relevant urls...\n",
      "Searching for relevant urls...\n",
      "Searching for relevant urls...\n",
      "INFO:langchain.retrievers.web_research:Search results: [{'title': 'German Civil Code BGB', 'link': 'https://www.gesetze-im-internet.de/englisch_bgb/englisch_bgb.html', 'snippet': 'If the endowment transaction does not satisfy the requirements of sentence 3 and if the founder is dead, then section 83 sentences 2 to 4 applies accordingly. (\\xa0...'}]\n",
      "Search results: [{'title': 'German Civil Code BGB', 'link': 'https://www.gesetze-im-internet.de/englisch_bgb/englisch_bgb.html', 'snippet': 'If the endowment transaction does not satisfy the requirements of sentence 3 and if the founder is dead, then section 83 sentences 2 to 4 applies accordingly. (\\xa0...'}]\n",
      "Search results: [{'title': 'German Civil Code BGB', 'link': 'https://www.gesetze-im-internet.de/englisch_bgb/englisch_bgb.html', 'snippet': 'If the endowment transaction does not satisfy the requirements of sentence 3 and if the founder is dead, then section 83 sentences 2 to 4 applies accordingly. (\\xa0...'}]\n",
      "Search results: [{'title': 'German Civil Code BGB', 'link': 'https://www.gesetze-im-internet.de/englisch_bgb/englisch_bgb.html', 'snippet': 'If the endowment transaction does not satisfy the requirements of sentence 3 and if the founder is dead, then section 83 sentences 2 to 4 applies accordingly. (\\xa0...'}]\n",
      "Search results: [{'title': 'German Civil Code BGB', 'link': 'https://www.gesetze-im-internet.de/englisch_bgb/englisch_bgb.html', 'snippet': 'If the endowment transaction does not satisfy the requirements of sentence 3 and if the founder is dead, then section 83 sentences 2 to 4 applies accordingly. (\\xa0...'}]\n",
      "Search results: [{'title': 'German Civil Code BGB', 'link': 'https://www.gesetze-im-internet.de/englisch_bgb/englisch_bgb.html', 'snippet': 'If the endowment transaction does not satisfy the requirements of sentence 3 and if the founder is dead, then section 83 sentences 2 to 4 applies accordingly. (\\xa0...'}]\n",
      "INFO:langchain.retrievers.web_research:Searching for relevant urls...\n",
      "Searching for relevant urls...\n",
      "Searching for relevant urls...\n",
      "Searching for relevant urls...\n",
      "Searching for relevant urls...\n",
      "Searching for relevant urls...\n",
      "INFO:langchain.retrievers.web_research:Search results: [{'title': 'Official Journal C 271A/14 J', 'link': 'https://eur-lex.europa.eu/legal-content/EN/TXT/HTML/?uri=OJ:C:2022:271A:FULL&from=EN', 'snippet': '2. WHAT DUTIES CAN I EXPECT TO PERFORM? 3. 3. AM I ELIGIBLE? 3\\xa0...'}]\n",
      "Search results: [{'title': 'Official Journal C 271A/14 J', 'link': 'https://eur-lex.europa.eu/legal-content/EN/TXT/HTML/?uri=OJ:C:2022:271A:FULL&from=EN', 'snippet': '2. WHAT DUTIES CAN I EXPECT TO PERFORM? 3. 3. AM I ELIGIBLE? 3\\xa0...'}]\n",
      "Search results: [{'title': 'Official Journal C 271A/14 J', 'link': 'https://eur-lex.europa.eu/legal-content/EN/TXT/HTML/?uri=OJ:C:2022:271A:FULL&from=EN', 'snippet': '2. WHAT DUTIES CAN I EXPECT TO PERFORM? 3. 3. AM I ELIGIBLE? 3\\xa0...'}]\n",
      "Search results: [{'title': 'Official Journal C 271A/14 J', 'link': 'https://eur-lex.europa.eu/legal-content/EN/TXT/HTML/?uri=OJ:C:2022:271A:FULL&from=EN', 'snippet': '2. WHAT DUTIES CAN I EXPECT TO PERFORM? 3. 3. AM I ELIGIBLE? 3\\xa0...'}]\n",
      "Search results: [{'title': 'Official Journal C 271A/14 J', 'link': 'https://eur-lex.europa.eu/legal-content/EN/TXT/HTML/?uri=OJ:C:2022:271A:FULL&from=EN', 'snippet': '2. WHAT DUTIES CAN I EXPECT TO PERFORM? 3. 3. AM I ELIGIBLE? 3\\xa0...'}]\n",
      "Search results: [{'title': 'Official Journal C 271A/14 J', 'link': 'https://eur-lex.europa.eu/legal-content/EN/TXT/HTML/?uri=OJ:C:2022:271A:FULL&from=EN', 'snippet': '2. WHAT DUTIES CAN I EXPECT TO PERFORM? 3. 3. AM I ELIGIBLE? 3\\xa0...'}]\n",
      "INFO:langchain.retrievers.web_research:New URLs to load: []\n",
      "New URLs to load: []\n",
      "New URLs to load: []\n",
      "New URLs to load: []\n",
      "New URLs to load: []\n",
      "New URLs to load: []\n",
      "INFO:langchain.retrievers.web_research:Grabbing most relevant splits from urls...\n",
      "Grabbing most relevant splits from urls...\n",
      "Grabbing most relevant splits from urls...\n",
      "Grabbing most relevant splits from urls...\n",
      "Grabbing most relevant splits from urls...\n",
      "Grabbing most relevant splits from urls...\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': \"This model's maximum context length is 4097 tokens. However, your messages resulted in 4295 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m/home/mz/code/MaCoZu/bureaubot/tool_agent.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mz/code/MaCoZu/bureaubot/tool_agent.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m multi_tool_executor\u001b[39m.\u001b[39;49mrun(\u001b[39m\"\u001b[39;49m\u001b[39mWhere can I find information about the duties of landlords?\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/micromamba/envs/bureau/lib/python3.10/site-packages/langchain/chains/base.py:505\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[1;32m    503\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    504\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m`run` supports only one positional argument.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 505\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(args[\u001b[39m0\u001b[39;49m], callbacks\u001b[39m=\u001b[39;49mcallbacks, tags\u001b[39m=\u001b[39;49mtags, metadata\u001b[39m=\u001b[39;49mmetadata)[\n\u001b[1;32m    506\u001b[0m         _output_key\n\u001b[1;32m    507\u001b[0m     ]\n\u001b[1;32m    509\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[1;32m    510\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m(kwargs, callbacks\u001b[39m=\u001b[39mcallbacks, tags\u001b[39m=\u001b[39mtags, metadata\u001b[39m=\u001b[39mmetadata)[\n\u001b[1;32m    511\u001b[0m         _output_key\n\u001b[1;32m    512\u001b[0m     ]\n",
      "File \u001b[0;32m~/micromamba/envs/bureau/lib/python3.10/site-packages/langchain/chains/base.py:310\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    309\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 310\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    311\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    312\u001b[0m final_outputs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(\n\u001b[1;32m    313\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    314\u001b[0m )\n",
      "File \u001b[0;32m~/micromamba/envs/bureau/lib/python3.10/site-packages/langchain/chains/base.py:304\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    297\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[1;32m    298\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[1;32m    299\u001b[0m     inputs,\n\u001b[1;32m    300\u001b[0m     name\u001b[39m=\u001b[39mrun_name,\n\u001b[1;32m    301\u001b[0m )\n\u001b[1;32m    302\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    303\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 304\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[1;32m    305\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    306\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[1;32m    307\u001b[0m     )\n\u001b[1;32m    308\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    309\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/micromamba/envs/bureau/lib/python3.10/site-packages/langchain/agents/agent.py:1167\u001b[0m, in \u001b[0;36mAgentExecutor._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m   1165\u001b[0m \u001b[39m# We now enter the agent loop (until it returns something).\u001b[39;00m\n\u001b[1;32m   1166\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_continue(iterations, time_elapsed):\n\u001b[0;32m-> 1167\u001b[0m     next_step_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_take_next_step(\n\u001b[1;32m   1168\u001b[0m         name_to_tool_map,\n\u001b[1;32m   1169\u001b[0m         color_mapping,\n\u001b[1;32m   1170\u001b[0m         inputs,\n\u001b[1;32m   1171\u001b[0m         intermediate_steps,\n\u001b[1;32m   1172\u001b[0m         run_manager\u001b[39m=\u001b[39;49mrun_manager,\n\u001b[1;32m   1173\u001b[0m     )\n\u001b[1;32m   1174\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[1;32m   1175\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_return(\n\u001b[1;32m   1176\u001b[0m             next_step_output, intermediate_steps, run_manager\u001b[39m=\u001b[39mrun_manager\n\u001b[1;32m   1177\u001b[0m         )\n",
      "File \u001b[0;32m~/micromamba/envs/bureau/lib/python3.10/site-packages/langchain/agents/agent.py:1017\u001b[0m, in \u001b[0;36mAgentExecutor._take_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m   1015\u001b[0m         tool_run_kwargs[\u001b[39m\"\u001b[39m\u001b[39mllm_prefix\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1016\u001b[0m     \u001b[39m# We then call the tool on the tool input to get an observation\u001b[39;00m\n\u001b[0;32m-> 1017\u001b[0m     observation \u001b[39m=\u001b[39m tool\u001b[39m.\u001b[39;49mrun(\n\u001b[1;32m   1018\u001b[0m         agent_action\u001b[39m.\u001b[39;49mtool_input,\n\u001b[1;32m   1019\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[1;32m   1020\u001b[0m         color\u001b[39m=\u001b[39;49mcolor,\n\u001b[1;32m   1021\u001b[0m         callbacks\u001b[39m=\u001b[39;49mrun_manager\u001b[39m.\u001b[39;49mget_child() \u001b[39mif\u001b[39;49;00m run_manager \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m   1022\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mtool_run_kwargs,\n\u001b[1;32m   1023\u001b[0m     )\n\u001b[1;32m   1024\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1025\u001b[0m     tool_run_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39magent\u001b[39m.\u001b[39mtool_run_logging_kwargs()\n",
      "File \u001b[0;32m~/micromamba/envs/bureau/lib/python3.10/site-packages/langchain/tools/base.py:365\u001b[0m, in \u001b[0;36mBaseTool.run\u001b[0;34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mException\u001b[39;00m, \u001b[39mKeyboardInterrupt\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    364\u001b[0m     run_manager\u001b[39m.\u001b[39mon_tool_error(e)\n\u001b[0;32m--> 365\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    366\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    367\u001b[0m     run_manager\u001b[39m.\u001b[39mon_tool_end(\n\u001b[1;32m    368\u001b[0m         \u001b[39mstr\u001b[39m(observation), color\u001b[39m=\u001b[39mcolor, name\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    369\u001b[0m     )\n",
      "File \u001b[0;32m~/micromamba/envs/bureau/lib/python3.10/site-packages/langchain/tools/base.py:337\u001b[0m, in \u001b[0;36mBaseTool.run\u001b[0;34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    335\u001b[0m     tool_args, tool_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_to_args_and_kwargs(parsed_input)\n\u001b[1;32m    336\u001b[0m     observation \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 337\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(\u001b[39m*\u001b[39;49mtool_args, run_manager\u001b[39m=\u001b[39;49mrun_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mtool_kwargs)\n\u001b[1;32m    338\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    339\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run(\u001b[39m*\u001b[39mtool_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtool_kwargs)\n\u001b[1;32m    340\u001b[0m     )\n\u001b[1;32m    341\u001b[0m \u001b[39mexcept\u001b[39;00m ToolException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    342\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_tool_error:\n",
      "File \u001b[0;32m~/micromamba/envs/bureau/lib/python3.10/site-packages/langchain/tools/base.py:516\u001b[0m, in \u001b[0;36mTool._run\u001b[0;34m(self, run_manager, *args, **kwargs)\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunc:\n\u001b[1;32m    508\u001b[0m     new_argument_supported \u001b[39m=\u001b[39m signature(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunc)\u001b[39m.\u001b[39mparameters\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mcallbacks\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    509\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    510\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunc(\n\u001b[1;32m    511\u001b[0m             \u001b[39m*\u001b[39margs,\n\u001b[1;32m    512\u001b[0m             callbacks\u001b[39m=\u001b[39mrun_manager\u001b[39m.\u001b[39mget_child() \u001b[39mif\u001b[39;00m run_manager \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    513\u001b[0m             \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    514\u001b[0m         )\n\u001b[1;32m    515\u001b[0m         \u001b[39mif\u001b[39;00m new_argument_supported\n\u001b[0;32m--> 516\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunc(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    517\u001b[0m     )\n\u001b[1;32m    518\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTool does not support sync\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/home/mz/code/MaCoZu/bureaubot/tool_agent.ipynb Cell 14\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mz/code/MaCoZu/bureaubot/tool_agent.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m index \u001b[39m=\u001b[39m load_index_from_storage(storage_context)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mz/code/MaCoZu/bureaubot/tool_agent.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m query_engine \u001b[39m=\u001b[39m index\u001b[39m.\u001b[39mas_query_engine(\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mz/code/MaCoZu/bureaubot/tool_agent.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     streaming\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, \n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mz/code/MaCoZu/bureaubot/tool_agent.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     text_qa_template\u001b[39m=\u001b[39mqa_template, \n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mz/code/MaCoZu/bureaubot/tool_agent.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     similarity_top_k\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mz/code/MaCoZu/bureaubot/tool_agent.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mz/code/MaCoZu/bureaubot/tool_agent.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m tools \u001b[39m=\u001b[39m [\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mz/code/MaCoZu/bureaubot/tool_agent.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     Tool(\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mz/code/MaCoZu/bureaubot/tool_agent.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m         name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLlamaIndex\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mz/code/MaCoZu/bureaubot/tool_agent.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m         func\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m q: \u001b[39mstr\u001b[39m(query_engine\u001b[39m.\u001b[39mquery(q)),\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mz/code/MaCoZu/bureaubot/tool_agent.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m         description\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39museful for when you want to answer questions about work and unemployment. The input to this tool should be a complete english sentence.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mz/code/MaCoZu/bureaubot/tool_agent.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m         return_direct\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mz/code/MaCoZu/bureaubot/tool_agent.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m     ),\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mz/code/MaCoZu/bureaubot/tool_agent.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m     Tool(\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mz/code/MaCoZu/bureaubot/tool_agent.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m         name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mWebsearch\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mz/code/MaCoZu/bureaubot/tool_agent.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m         func\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m q: \u001b[39mstr\u001b[39m(qa_chain({\u001b[39m\"\u001b[39;49m\u001b[39mquestion\u001b[39;49m\u001b[39m\"\u001b[39;49m: q})),\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mz/code/MaCoZu/bureaubot/tool_agent.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m         description\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39museful for when you want to answer questions about work and unemployment and the LlamaIndex Tool does not have a sufficient answer in it\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms documents. The input to this tool should be a complete english sentence.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mz/code/MaCoZu/bureaubot/tool_agent.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m         return_direct\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mz/code/MaCoZu/bureaubot/tool_agent.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m     ),\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mz/code/MaCoZu/bureaubot/tool_agent.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m ]\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mz/code/MaCoZu/bureaubot/tool_agent.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m# Re-initialize the agent with our new list of tools\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mz/code/MaCoZu/bureaubot/tool_agent.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m prompt_with_history \u001b[39m=\u001b[39m CustomPromptTemplate(\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mz/code/MaCoZu/bureaubot/tool_agent.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m     template\u001b[39m=\u001b[39mtemplate_with_history,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mz/code/MaCoZu/bureaubot/tool_agent.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m     tools\u001b[39m=\u001b[39mtools,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mz/code/MaCoZu/bureaubot/tool_agent.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m     input_variables\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39minput\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mintermediate_steps\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mhistory\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mz/code/MaCoZu/bureaubot/tool_agent.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m )\n",
      "File \u001b[0;32m~/micromamba/envs/bureau/lib/python3.10/site-packages/langchain/chains/base.py:310\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    309\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 310\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    311\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    312\u001b[0m final_outputs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(\n\u001b[1;32m    313\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    314\u001b[0m )\n",
      "File \u001b[0;32m~/micromamba/envs/bureau/lib/python3.10/site-packages/langchain/chains/base.py:304\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    297\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[1;32m    298\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[1;32m    299\u001b[0m     inputs,\n\u001b[1;32m    300\u001b[0m     name\u001b[39m=\u001b[39mrun_name,\n\u001b[1;32m    301\u001b[0m )\n\u001b[1;32m    302\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    303\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 304\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[1;32m    305\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    306\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[1;32m    307\u001b[0m     )\n\u001b[1;32m    308\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    309\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/micromamba/envs/bureau/lib/python3.10/site-packages/langchain/chains/qa_with_sources/base.py:155\u001b[0m, in \u001b[0;36mBaseQAWithSourcesChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    153\u001b[0m     docs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_docs(inputs)  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 155\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcombine_documents_chain\u001b[39m.\u001b[39;49mrun(\n\u001b[1;32m    156\u001b[0m     input_documents\u001b[39m=\u001b[39;49mdocs, callbacks\u001b[39m=\u001b[39;49m_run_manager\u001b[39m.\u001b[39;49mget_child(), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs\n\u001b[1;32m    157\u001b[0m )\n\u001b[1;32m    158\u001b[0m answer, sources \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_split_sources(answer)\n\u001b[1;32m    159\u001b[0m result: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m {\n\u001b[1;32m    160\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39manswer_key: answer,\n\u001b[1;32m    161\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msources_answer_key: sources,\n\u001b[1;32m    162\u001b[0m }\n",
      "File \u001b[0;32m~/micromamba/envs/bureau/lib/python3.10/site-packages/langchain/chains/base.py:510\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[1;32m    505\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m(args[\u001b[39m0\u001b[39m], callbacks\u001b[39m=\u001b[39mcallbacks, tags\u001b[39m=\u001b[39mtags, metadata\u001b[39m=\u001b[39mmetadata)[\n\u001b[1;32m    506\u001b[0m         _output_key\n\u001b[1;32m    507\u001b[0m     ]\n\u001b[1;32m    509\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[0;32m--> 510\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(kwargs, callbacks\u001b[39m=\u001b[39;49mcallbacks, tags\u001b[39m=\u001b[39;49mtags, metadata\u001b[39m=\u001b[39;49mmetadata)[\n\u001b[1;32m    511\u001b[0m         _output_key\n\u001b[1;32m    512\u001b[0m     ]\n\u001b[1;32m    514\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[1;32m    515\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    516\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`run` supported with either positional arguments or keyword arguments,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    517\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m but none were provided.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    518\u001b[0m     )\n",
      "File \u001b[0;32m~/micromamba/envs/bureau/lib/python3.10/site-packages/langchain/chains/base.py:310\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    309\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 310\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    311\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    312\u001b[0m final_outputs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(\n\u001b[1;32m    313\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    314\u001b[0m )\n",
      "File \u001b[0;32m~/micromamba/envs/bureau/lib/python3.10/site-packages/langchain/chains/base.py:304\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    297\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[1;32m    298\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[1;32m    299\u001b[0m     inputs,\n\u001b[1;32m    300\u001b[0m     name\u001b[39m=\u001b[39mrun_name,\n\u001b[1;32m    301\u001b[0m )\n\u001b[1;32m    302\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    303\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 304\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[1;32m    305\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    306\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[1;32m    307\u001b[0m     )\n\u001b[1;32m    308\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    309\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/micromamba/envs/bureau/lib/python3.10/site-packages/langchain/chains/combine_documents/base.py:122\u001b[0m, in \u001b[0;36mBaseCombineDocumentsChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[39m# Other keys are assumed to be needed for LLM prediction\u001b[39;00m\n\u001b[1;32m    121\u001b[0m other_keys \u001b[39m=\u001b[39m {k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m inputs\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_key}\n\u001b[0;32m--> 122\u001b[0m output, extra_return_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcombine_docs(\n\u001b[1;32m    123\u001b[0m     docs, callbacks\u001b[39m=\u001b[39;49m_run_manager\u001b[39m.\u001b[39;49mget_child(), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mother_keys\n\u001b[1;32m    124\u001b[0m )\n\u001b[1;32m    125\u001b[0m extra_return_dict[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_key] \u001b[39m=\u001b[39m output\n\u001b[1;32m    126\u001b[0m \u001b[39mreturn\u001b[39;00m extra_return_dict\n",
      "File \u001b[0;32m~/micromamba/envs/bureau/lib/python3.10/site-packages/langchain/chains/combine_documents/stuff.py:171\u001b[0m, in \u001b[0;36mStuffDocumentsChain.combine_docs\u001b[0;34m(self, docs, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    169\u001b[0m inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_inputs(docs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    170\u001b[0m \u001b[39m# Call predict on the LLM.\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mllm_chain\u001b[39m.\u001b[39;49mpredict(callbacks\u001b[39m=\u001b[39;49mcallbacks, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs), {}\n",
      "File \u001b[0;32m~/micromamba/envs/bureau/lib/python3.10/site-packages/langchain/chains/llm.py:298\u001b[0m, in \u001b[0;36mLLMChain.predict\u001b[0;34m(self, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, callbacks: Callbacks \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[1;32m    284\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Format prompt with kwargs and pass to LLM.\u001b[39;00m\n\u001b[1;32m    285\u001b[0m \n\u001b[1;32m    286\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[39m            completion = llm.predict(adjective=\"funny\")\u001b[39;00m\n\u001b[1;32m    297\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 298\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(kwargs, callbacks\u001b[39m=\u001b[39;49mcallbacks)[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_key]\n",
      "File \u001b[0;32m~/micromamba/envs/bureau/lib/python3.10/site-packages/langchain/chains/base.py:310\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    309\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 310\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    311\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    312\u001b[0m final_outputs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(\n\u001b[1;32m    313\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    314\u001b[0m )\n",
      "File \u001b[0;32m~/micromamba/envs/bureau/lib/python3.10/site-packages/langchain/chains/base.py:304\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    297\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[1;32m    298\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[1;32m    299\u001b[0m     inputs,\n\u001b[1;32m    300\u001b[0m     name\u001b[39m=\u001b[39mrun_name,\n\u001b[1;32m    301\u001b[0m )\n\u001b[1;32m    302\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    303\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 304\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[1;32m    305\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    306\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[1;32m    307\u001b[0m     )\n\u001b[1;32m    308\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    309\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/micromamba/envs/bureau/lib/python3.10/site-packages/langchain/chains/llm.py:108\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call\u001b[39m(\n\u001b[1;32m    104\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    105\u001b[0m     inputs: Dict[\u001b[39mstr\u001b[39m, Any],\n\u001b[1;32m    106\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    107\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[0;32m--> 108\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate([inputs], run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[1;32m    109\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_outputs(response)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/micromamba/envs/bureau/lib/python3.10/site-packages/langchain/chains/llm.py:120\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[0;34m(self, input_list, run_manager)\u001b[0m\n\u001b[1;32m    118\u001b[0m callbacks \u001b[39m=\u001b[39m run_manager\u001b[39m.\u001b[39mget_child() \u001b[39mif\u001b[39;00m run_manager \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm, BaseLanguageModel):\n\u001b[0;32m--> 120\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mllm\u001b[39m.\u001b[39;49mgenerate_prompt(\n\u001b[1;32m    121\u001b[0m         prompts,\n\u001b[1;32m    122\u001b[0m         stop,\n\u001b[1;32m    123\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    124\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mllm_kwargs,\n\u001b[1;32m    125\u001b[0m     )\n\u001b[1;32m    126\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    127\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm\u001b[39m.\u001b[39mbind(stop\u001b[39m=\u001b[39mstop, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm_kwargs)\u001b[39m.\u001b[39mbatch(\n\u001b[1;32m    128\u001b[0m         cast(List, prompts), {\u001b[39m\"\u001b[39m\u001b[39mcallbacks\u001b[39m\u001b[39m\"\u001b[39m: callbacks}\n\u001b[1;32m    129\u001b[0m     )\n",
      "File \u001b[0;32m~/micromamba/envs/bureau/lib/python3.10/site-packages/langchain/chat_models/base.py:459\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_prompt\u001b[39m(\n\u001b[1;32m    452\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    453\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    457\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[1;32m    458\u001b[0m     prompt_messages \u001b[39m=\u001b[39m [p\u001b[39m.\u001b[39mto_messages() \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m prompts]\n\u001b[0;32m--> 459\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate(prompt_messages, stop\u001b[39m=\u001b[39;49mstop, callbacks\u001b[39m=\u001b[39;49mcallbacks, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/micromamba/envs/bureau/lib/python3.10/site-packages/langchain/chat_models/base.py:349\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[39mif\u001b[39;00m run_managers:\n\u001b[1;32m    348\u001b[0m             run_managers[i]\u001b[39m.\u001b[39mon_llm_error(e)\n\u001b[0;32m--> 349\u001b[0m         \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    350\u001b[0m flattened_outputs \u001b[39m=\u001b[39m [\n\u001b[1;32m    351\u001b[0m     LLMResult(generations\u001b[39m=\u001b[39m[res\u001b[39m.\u001b[39mgenerations], llm_output\u001b[39m=\u001b[39mres\u001b[39m.\u001b[39mllm_output)\n\u001b[1;32m    352\u001b[0m     \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m results\n\u001b[1;32m    353\u001b[0m ]\n\u001b[1;32m    354\u001b[0m llm_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_combine_llm_outputs([res\u001b[39m.\u001b[39mllm_output \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m results])\n",
      "File \u001b[0;32m~/micromamba/envs/bureau/lib/python3.10/site-packages/langchain/chat_models/base.py:339\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[39mfor\u001b[39;00m i, m \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(messages):\n\u001b[1;32m    337\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    338\u001b[0m         results\u001b[39m.\u001b[39mappend(\n\u001b[0;32m--> 339\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate_with_cache(\n\u001b[1;32m    340\u001b[0m                 m,\n\u001b[1;32m    341\u001b[0m                 stop\u001b[39m=\u001b[39;49mstop,\n\u001b[1;32m    342\u001b[0m                 run_manager\u001b[39m=\u001b[39;49mrun_managers[i] \u001b[39mif\u001b[39;49;00m run_managers \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    343\u001b[0m                 \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    344\u001b[0m             )\n\u001b[1;32m    345\u001b[0m         )\n\u001b[1;32m    346\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    347\u001b[0m         \u001b[39mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/micromamba/envs/bureau/lib/python3.10/site-packages/langchain/chat_models/base.py:492\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    488\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    489\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    490\u001b[0m     )\n\u001b[1;32m    491\u001b[0m \u001b[39mif\u001b[39;00m new_arg_supported:\n\u001b[0;32m--> 492\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate(\n\u001b[1;32m    493\u001b[0m         messages, stop\u001b[39m=\u001b[39;49mstop, run_manager\u001b[39m=\u001b[39;49mrun_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    494\u001b[0m     )\n\u001b[1;32m    495\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    496\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(messages, stop\u001b[39m=\u001b[39mstop, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/micromamba/envs/bureau/lib/python3.10/site-packages/langchain/chat_models/openai.py:417\u001b[0m, in \u001b[0;36mChatOpenAI._generate\u001b[0;34m(self, messages, stop, run_manager, stream, **kwargs)\u001b[0m\n\u001b[1;32m    415\u001b[0m message_dicts, params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_message_dicts(messages, stop)\n\u001b[1;32m    416\u001b[0m params \u001b[39m=\u001b[39m {\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs}\n\u001b[0;32m--> 417\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompletion_with_retry(\n\u001b[1;32m    418\u001b[0m     messages\u001b[39m=\u001b[39;49mmessage_dicts, run_manager\u001b[39m=\u001b[39;49mrun_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams\n\u001b[1;32m    419\u001b[0m )\n\u001b[1;32m    420\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_chat_result(response)\n",
      "File \u001b[0;32m~/micromamba/envs/bureau/lib/python3.10/site-packages/langchain/chat_models/openai.py:339\u001b[0m, in \u001b[0;36mChatOpenAI.completion_with_retry\u001b[0;34m(self, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Use tenacity to retry the completion call.\"\"\"\u001b[39;00m\n\u001b[1;32m    338\u001b[0m \u001b[39mif\u001b[39;00m is_openai_v1():\n\u001b[0;32m--> 339\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclient\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    341\u001b[0m retry_decorator \u001b[39m=\u001b[39m _create_retry_decorator(\u001b[39mself\u001b[39m, run_manager\u001b[39m=\u001b[39mrun_manager)\n\u001b[1;32m    343\u001b[0m \u001b[39m@retry_decorator\u001b[39m\n\u001b[1;32m    344\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_completion_with_retry\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n",
      "File \u001b[0;32m~/micromamba/envs/bureau/lib/python3.10/site-packages/openai/_utils/_utils.py:299\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    297\u001b[0m             msg \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMissing required argument: \u001b[39m\u001b[39m{\u001b[39;00mquote(missing[\u001b[39m0\u001b[39m])\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    298\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 299\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/micromamba/envs/bureau/lib/python3.10/site-packages/openai/resources/chat/completions.py:594\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[39m@required_args\u001b[39m([\u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m], [\u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    549\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    550\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    592\u001b[0m     timeout: \u001b[39mfloat\u001b[39m \u001b[39m|\u001b[39m httpx\u001b[39m.\u001b[39mTimeout \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m|\u001b[39m NotGiven \u001b[39m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    593\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ChatCompletion \u001b[39m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 594\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_post(\n\u001b[1;32m    595\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39m/chat/completions\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    596\u001b[0m         body\u001b[39m=\u001b[39;49mmaybe_transform(\n\u001b[1;32m    597\u001b[0m             {\n\u001b[1;32m    598\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmessages\u001b[39;49m\u001b[39m\"\u001b[39;49m: messages,\n\u001b[1;32m    599\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmodel\u001b[39;49m\u001b[39m\"\u001b[39;49m: model,\n\u001b[1;32m    600\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfrequency_penalty\u001b[39;49m\u001b[39m\"\u001b[39;49m: frequency_penalty,\n\u001b[1;32m    601\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfunction_call\u001b[39;49m\u001b[39m\"\u001b[39;49m: function_call,\n\u001b[1;32m    602\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfunctions\u001b[39;49m\u001b[39m\"\u001b[39;49m: functions,\n\u001b[1;32m    603\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mlogit_bias\u001b[39;49m\u001b[39m\"\u001b[39;49m: logit_bias,\n\u001b[1;32m    604\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmax_tokens\u001b[39;49m\u001b[39m\"\u001b[39;49m: max_tokens,\n\u001b[1;32m    605\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mn\u001b[39;49m\u001b[39m\"\u001b[39;49m: n,\n\u001b[1;32m    606\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mpresence_penalty\u001b[39;49m\u001b[39m\"\u001b[39;49m: presence_penalty,\n\u001b[1;32m    607\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mresponse_format\u001b[39;49m\u001b[39m\"\u001b[39;49m: response_format,\n\u001b[1;32m    608\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mseed\u001b[39;49m\u001b[39m\"\u001b[39;49m: seed,\n\u001b[1;32m    609\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstop\u001b[39;49m\u001b[39m\"\u001b[39;49m: stop,\n\u001b[1;32m    610\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstream\u001b[39;49m\u001b[39m\"\u001b[39;49m: stream,\n\u001b[1;32m    611\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtemperature\u001b[39;49m\u001b[39m\"\u001b[39;49m: temperature,\n\u001b[1;32m    612\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtool_choice\u001b[39;49m\u001b[39m\"\u001b[39;49m: tool_choice,\n\u001b[1;32m    613\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtools\u001b[39;49m\u001b[39m\"\u001b[39;49m: tools,\n\u001b[1;32m    614\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtop_p\u001b[39;49m\u001b[39m\"\u001b[39;49m: top_p,\n\u001b[1;32m    615\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m: user,\n\u001b[1;32m    616\u001b[0m             },\n\u001b[1;32m    617\u001b[0m             completion_create_params\u001b[39m.\u001b[39;49mCompletionCreateParams,\n\u001b[1;32m    618\u001b[0m         ),\n\u001b[1;32m    619\u001b[0m         options\u001b[39m=\u001b[39;49mmake_request_options(\n\u001b[1;32m    620\u001b[0m             extra_headers\u001b[39m=\u001b[39;49mextra_headers, extra_query\u001b[39m=\u001b[39;49mextra_query, extra_body\u001b[39m=\u001b[39;49mextra_body, timeout\u001b[39m=\u001b[39;49mtimeout\n\u001b[1;32m    621\u001b[0m         ),\n\u001b[1;32m    622\u001b[0m         cast_to\u001b[39m=\u001b[39;49mChatCompletion,\n\u001b[1;32m    623\u001b[0m         stream\u001b[39m=\u001b[39;49mstream \u001b[39mor\u001b[39;49;00m \u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    624\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mStream[ChatCompletionChunk],\n\u001b[1;32m    625\u001b[0m     )\n",
      "File \u001b[0;32m~/micromamba/envs/bureau/lib/python3.10/site-packages/openai/_base_client.py:1055\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1041\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpost\u001b[39m(\n\u001b[1;32m   1042\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1043\u001b[0m     path: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1050\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1051\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[1;32m   1052\u001b[0m     opts \u001b[39m=\u001b[39m FinalRequestOptions\u001b[39m.\u001b[39mconstruct(\n\u001b[1;32m   1053\u001b[0m         method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m\"\u001b[39m, url\u001b[39m=\u001b[39mpath, json_data\u001b[39m=\u001b[39mbody, files\u001b[39m=\u001b[39mto_httpx_files(files), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions\n\u001b[1;32m   1054\u001b[0m     )\n\u001b[0;32m-> 1055\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(ResponseT, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest(cast_to, opts, stream\u001b[39m=\u001b[39;49mstream, stream_cls\u001b[39m=\u001b[39;49mstream_cls))\n",
      "File \u001b[0;32m~/micromamba/envs/bureau/lib/python3.10/site-packages/openai/_base_client.py:834\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    825\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    826\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    827\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    832\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    833\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[0;32m--> 834\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[1;32m    835\u001b[0m         cast_to\u001b[39m=\u001b[39;49mcast_to,\n\u001b[1;32m    836\u001b[0m         options\u001b[39m=\u001b[39;49moptions,\n\u001b[1;32m    837\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    838\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m    839\u001b[0m         remaining_retries\u001b[39m=\u001b[39;49mremaining_retries,\n\u001b[1;32m    840\u001b[0m     )\n",
      "File \u001b[0;32m~/micromamba/envs/bureau/lib/python3.10/site-packages/openai/_base_client.py:877\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    874\u001b[0m     \u001b[39m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m    875\u001b[0m     \u001b[39m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m    876\u001b[0m     err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mread()\n\u001b[0;32m--> 877\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_status_error_from_response(err\u001b[39m.\u001b[39mresponse) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    878\u001b[0m \u001b[39mexcept\u001b[39;00m httpx\u001b[39m.\u001b[39mTimeoutException \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    879\u001b[0m     \u001b[39mif\u001b[39;00m retries \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"This model's maximum context length is 4097 tokens. However, your messages resulted in 4295 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}"
     ]
    }
   ],
   "source": [
    "multi_tool_executor.run(\"Where can I find information about the duties of landlords?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tools as Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "from bot_utils import default_engine\n",
    "from dotenv import load_dotenv\n",
    "from langchain.agents import AgentType, initialize_agent\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "from langchain.chat_models.openai import ChatOpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.retrievers.web_research import WebResearchRetriever\n",
    "from langchain.tools import Tool\n",
    "from langchain.utilities import GoogleSearchAPIWrapper\n",
    "from langchain.vectorstores import Chroma\n",
    "from llama_index import Prompt\n",
    "from llama_index.langchain_helpers.agents import IndexToolConfig, LlamaIndexTool\n",
    "from langchain.tools import BaseTool\n",
    "from langchain.agents import tool\n",
    "\n",
    "# logging to see what sites and documents the web retriever is using\n",
    "logging.basicConfig()\n",
    "logging.getLogger(\"langchain.retrievers.web_research\").setLevel(logging.INFO)\n",
    "\n",
    "# Search\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "GOOGLE_CSE_ID = os.getenv(\"GOOGLE_CSE_ID\")\n",
    "\n",
    "search = GoogleSearchAPIWrapper()\n",
    "llm = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\")\n",
    "\n",
    "\n",
    "class EngineTool:\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.qa_template = Prompt(\n",
    "            \"We have provided context information below. \\n\"\n",
    "            \"---------------------\\n\"\n",
    "            \"{context_str}\"\n",
    "            \"\\n---------------------\\n\"\n",
    "            \"Do not give me an answer if it is not mentioned in the context as a fact. \\n\"\n",
    "            \"Given this information, please provide me with an answer to the following question:\\n{query_str}\\n\"\n",
    "        )\n",
    "\n",
    "        self.folder_with_index = \"vector_db\"\n",
    "        self.number_top_results = 5\n",
    "\n",
    "        self.default_engine = default_engine(\n",
    "            self.folder_with_index, self.qa_template, self.number_top_results\n",
    "        )\n",
    "\n",
    "        # ERROR: IndexToolConfig awaits an instance of BaseQueryEngine\n",
    "        self.tool_config = IndexToolConfig(\n",
    "            query_engine=default_engine,\n",
    "            name=f\"Vector Index\",\n",
    "            description=f\"Useful for when you want to answer queries about work and unemployment in Berlin, Germany\",\n",
    "            tool_kwargs={\"return_direct\": True},\n",
    "        )\n",
    "\n",
    "    def run(self):\n",
    "        return LlamaIndexTool.from_tool_config(self.tool_config)\n",
    "\n",
    "\n",
    "class RetrieverTool:\n",
    "    \"\"\"Tool to find information about work, unemployment, laws and adminstrative infromation in Berlin, Germany\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Initialize LLM\n",
    "        self.llm = llm\n",
    "\n",
    "        # Vectorstore\n",
    "        self.vectorstore = Chroma(\n",
    "            embedding_function=OpenAIEmbeddings(), persist_directory=\"./chroma_db_oai\"\n",
    "        )\n",
    "\n",
    "        # Initialize\n",
    "        self.web_research_retriever = WebResearchRetriever.from_llm(\n",
    "            vectorstore=self.vectorstore,\n",
    "            llm=self.llm,\n",
    "            search=search,\n",
    "        )\n",
    "\n",
    "    def run(self, user_input):\n",
    "        qa_chain = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "            self.llm, retriever=self.web_research_retriever\n",
    "        )\n",
    "\n",
    "        return qa_chain({\"question\": user_input})\n",
    "\n",
    "\n",
    "class ToolChainAgent:\n",
    "    def __init__(self):\n",
    "        self.llm = llm\n",
    "\n",
    "        # initialize tools\n",
    "        self.q_engine_fct = EngineTool().run()\n",
    "        self.retriever_fct = RetrieverTool(self.llm).run\n",
    "\n",
    "        self.tools = [\n",
    "            # embedding first\n",
    "            Tool(\n",
    "                name=\"query engine\",\n",
    "                description=\"use this tool first, to get anwsers about work and unemployment in Berlin, Germany\",\n",
    "                func=self.q_engine_fct,\n",
    "            ),\n",
    "            # web retriever goes through programmable search engine\n",
    "            Tool(\n",
    "                name=\"web_retriever\",\n",
    "                description=\"use this tool when the first tool failed and you want to answer questions about work and unempoyment in Berlin, Germany\",\n",
    "                func=self.retriever_fct,\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "        self.agent = initialize_agent(\n",
    "            self.tools,\n",
    "            self.llm,\n",
    "            agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "            verbose=True,\n",
    "        )\n",
    "\n",
    "    def run(self, user_input):\n",
    "        \"\"\"\n",
    "        Agent runs query with the listed tools:\n",
    "\n",
    "        1. query engine from custom embedding\n",
    "        2. web retriever, which goes through programmable google-search\n",
    "\n",
    "        Run the agent instance like this:\n",
    "        agent = ToolChainAgent()\n",
    "        agent.run(promt)\n",
    "        \"\"\"\n",
    "        return self.agent(user_input)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
